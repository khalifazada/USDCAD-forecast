\documentclass[10pt]{report}
\usepackage{extsizes}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{color}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage[margin=0.75in]{geometry}

\begin{document}
\title{STAT 306: Assignment}
\author{ Emad Zadegan: 27452119\\ Michael Nguyen: 38223111\\ Chingiz Khalifazadeh: 21059134\\Kevin Zhu: 38726139 }
\date{April 7th, 2016}
\maketitle

\SweaveOpts{concordance=TRUE}

\section{Abstract/Summary}

The goal of this report is to form a prediction equation for the next day closing price of the US Dollar/Canadian Dollar exchange rate. We gathered data from investing.com as well as the MetaTrader trading platform. We used SQL to join the data across sources based on date, as well as form variables such as the volatility variables (named \texttt{... volatility} in R code) and the response variable, which is the US Dollar/Canadian Dollar exchange rate of the following day (called \texttt{response} in the R code). We omitted the details of joining and creation of variables code for sake of brevity for the reader.

\section{Initial Analysis}
Here we analyze the raw data to gain intuition on any scaling or transformations that may need to be performed before attempting model fitting.

<<>>=           
dat = read.table('matrix_final_5.csv', sep='\t', header=T)
# remove the last row since it does not have a valid response entry
# since we did not collect the US Dollar/Canadian Dollar exchange 
# rate of the  following date for the last entry
dat=dat[-dim(dat)[1],]

names(dat)
@

\subsection{Summary Statistics}

We choose to only work with a subset of the data, due to a lot of variables in the original data set being intuitively correlated such as the high, low and close price of a particular index on a given day.

<<>>==
subset_expl=dat[,c(2,5,6,7,10,11,12,15,16,19,20,21,22,25)]
summary(subset_expl)
@

\subsection{Description of Data}
We collect data from various within the time range of January $1^\text{st}$ 2013, to January $28^\text{th}$ 2016. As explained before the original set of variables has been subsetted due to many variables being from the same source.
\begin{center}
\begin{tabular}{ | c | c | }
\hline
Variables&explanation of units\\
\hline
EUR/USD Close&Exchange rate of Euro to the US dollar\\
\hline
Overnight Rate&Overnight interbank lending rate\\
\hline
S\&P500 Close&Standard \& Poor's index of 500 top US \\&companies by market capitalization.\\
\hline
S\&P TSX Close&Standard \& Poor's index of top Canadian\\&companies by market capitalization.\\
\hline
S\&P TSX Volume&Volume of contracts traded on the TSX\\&during the trading day in millions of units\\
\hline
WTI Crude Oil Spot Price&Closing price for a contract specifying\\&the price of a barrel of crude oil commodity.\\
\hline
Yuan/USD Close&Exchange rate of Chinese Yuan to US dollar\\
\hline
EUR/USD, S\&P500, S\&PTSX, USD/CAD Volatility&Difference of High \& Low\\&within a day, in market points\\
\hline
\end{tabular}
\end{center}

The next two figures cover visual aids for the data.

\begin{figure}
\SweaveOpts{width=12,height=15}
\begin{center}
%\includegraphics[width=\columnwidth]{boxplots}
<<boxplots,fig=TRUE,include=TRUE, echo=FALSE>>=  
par(mfrow=c(3,3))
par(mar=c(3,3,3,3))
for(i in 1:9)
{
  boxplot(subset_expl[,i])
  title(names(subset_expl)[i])
}
@
\end{center}
\caption{Box plot for the first response and the first 8 explanatory variables}
\end{figure}

\begin{figure}
\SweaveOpts{width=12,height=10}
\begin{center}
%\includegraphics[width=\columnwidth]{boxplots}
<<boxplots2,fig=TRUE,include=TRUE, echo=FALSE>>=  
par(mfrow=c(2,3))
par(mar=c(3,3,3,3))
for(i in 10:dim(subset_expl)[2])
{
  boxplot(subset_expl[,i])
  title(names(subset_expl)[i])
}
@
\end{center}
\caption{Boxplots for 5 explanatory variables}
\end{figure}

\begin{figure}
\begin{center}
%\includegraphics[width=\columnwidth]{histplots}
<<histplots,fig=TRUE,include=TRUE, echo=FALSE>>=  
par(mfrow=c(4,3))
par(mar=c(3,3,3,3))
for(i in 1:dim(subset_expl)[2])
{
  hist(subset_expl[,i], main=names(subset_expl)[i])
}
@
\end{center}
\caption{Histogram of response and all explanatory variables}
\end{figure}

\newpage

\subsection{Correlation Matrix}
% \begin{center}
% \begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | c | c | c | }
% \hline
% &1&2&3&4&5&6&7&8&9&10&11&12\\
% \hline
% \hline
% \end{tabular}
% \end{center}

Correlation between response and all explanatory variables. We notice very high correlation between the response and USD/CAD closing exchange rate, this is explained from the response being created through a shifting of USD/CAD closing exchange rate. There is also strong negative correlation between oil prices (\texttt{wti.crude.oil.spot.close}) and the response; this could imply an inverse relationship between US/Canadian exchange rate and oil prices in US Dollars.

<<>>==
mat=matrix(cor(subset_expl),ncol=dim(subset_expl)[2])
mat
# summary statistics of the matrix
# removed values the diagonal values, which are the only values equal to 1
mat_vals=c(mat)[which(c(mat) != 1)]
summary(mat_vals)
@
where each number represents, the $i$th column of the subset data set or again:

<<>>==
names(subset_expl)
@

\subsection{Initial Analysis Summary}
After summary statistics and intial analysis the following scaling was done to avoid small regression coefficients. We attempted to scale all variables to be in the range 0 and 10.

<<echo=FALSE>>==
subset_expl=dat[,c(1,2,5,6,7,10,11,12,15,16,19,20,21,22,25)]
@

<<>>==
tdat=subset_expl
tdat$EUR.USD.volatility=subset_expl$EUR.USD.volatility * 100
tdat$SP500.volatility=subset_expl$SP500.volatility / 10
tdat$SP500.Close=subset_expl$SP500.Close / 1000
tdat$SPTSX.volatility=subset_expl$SPTSX.volatility / 100
tdat$SPTSX.Close=subset_expl$SPTSX.Close / 10000
tdat$SPTSX.Volume.in.Millions=subset_expl$SPTSX.Volume.in.Millions / 100
tdat$USD.CAD.volatility=subset_expl$USD.CAD.volatility * 100
tdat$wti.crude.oil.spot.close=subset_expl$wti.crude.oil.spot.close / 100
tdat$YUAN.USD.Close=subset_expl$YUAN.USD.Close / 10
tdat$YUAN.USD.volatility=subset_expl$YUAN.USD.volatility * 100
@

As a result, new summary statistics:

<<>>==
summary(tdat)
@


\subsection{Line Charts}

To gain intuition on the trend of the explanatory variables against date, we use line charts.

<<echo=FALSE>>==
date=as.Date(dat$Date,"%Y-%m-%d")
n<-names(tdat)

plot_linecharts = function(start,end)
{
  par(mfrow=c(end-start+1,1))
  for (i in start:end)
  {
    if (i==start)
    {
      par(mar=c(0,4,1,4))
      plot(date,tdat[,i], xaxt='n', xlab="", ylab=paste(n[i]), type='l')
    }
    else if (i!=end)
    {
      par(mar=c(0,4,0,4))
      plot(date,tdat[,i], xaxt='n', xlab="", ylab=paste(n[i]), type='l')
    }
    else if(i==end)
    {
      par(mar=c(2.5,4,0,4))
      plot(date,tdat[,i], xlab="Date", ylab=paste(n[i]), type='l')
    }
  }
}
@

\SweaveOpts{width=12,height=15}
\begin{center}
<<linecharts1,fig=TRUE,include=TRUE, echo=FALSE>>=  
plot_linecharts(2,8)
@
\end{center}

\SweaveOpts{width=12,height=15}
\begin{center}
<<linecharts2,fig=TRUE,include=TRUE, echo=FALSE>>=  
plot_linecharts(9,dim(tdat)[2])
@
\end{center}

\section{Variable Selection}

We perform an exhaustive search to find the best model, based on linear order.

<<echo=FALSE>>==
tdatback = tdat
tdat = tdat[,-1]
@

<<>>==
# full model adjR2
summary(lm(tdat$response~.,data=tdat))$adj.r.squared

library(leaps)
s1<- regsubsets(tdat$response~., data=tdat, method="exhaustive")
ss1 <- summary(s1)

# adrR2 and cp of best model based on adjR2
ss1$adjr2[which.max(ss1$adjr2)]
ss1$cp[which.max(ss1$adjr2)]
# here we extract the model which gave the highest adjusted R^2
# then in the same line we draw the indices of the explanatory variables of this model
# then we draw a vector of the names from the indices for easy human interpretation
modeladj=names(tdat)[c(which(ss1$which[which.max(ss1$adjr2),] %in% TRUE))]
modeladj

# adrR2 and cp of best model based on cp
ss1$cp[which.min(ss1$cp)]
ss1$adjr2[which.min(ss1$cp)]
modelcp=names(tdat)[c(which(ss1$which[which.min(ss1$cp),] %in% TRUE))]
modelcp

# adjusted R^2 of full model
summary(lm(tdat$response~.,data=tdat))$adj.r.squared

# adjusted R^2 with a quadratic term
summary(lm(tdat$response~.+tdat$USD.CAD.Close^2,data=tdat))$adj.r.squared
@

We find that based on $C_p$ and adjusted $R^2$, both agree on a common best model. Also we did not find adding quadratic terms to the models could increase the adjusted $R^2$.

\subsection{Residual Analysis}

We plot residuals against fitted, as well as residuals against all explanatory variables.

\SweaveOpts{width=14,height=15}
\begin{center}
<<fig=TRUE,include=TRUE,echo=FALSE>>=
regsubset=tdat[,modeladj]
fit=lm(regsubset$response~.,data=regsubset)

par(mfrow=c(3,3))

par(mar=c(4,4.5,1,2))
plot(fit$fitted.values, fit$residuals, xlab="fitted", ylab="residuals")
for(i in 2:dim(regsubset)[2])
{
  plot(regsubset[,i], fit$residuals,xlab=names(regsubset)[i], ylab="residuals")
}
@
\end{center}

We can see an issue with the homoscedasticity on the S\&P 500 Close and residuals, but even with log, square root and log of log transforms we are unable to resolve the issue.


\subsection{Prediction Intervals}
<<echo=FALSE>>==
tdat = tdatback
@

<<echo=FALSE>>= FUNCTIONS: regress, looper, plotter
  # REGRESS
  # start.date: string, Date, format = "YYYY-MM-DD"
  # e.g. "2016-01-25"
  # var.names: vector of strings
  # dat: data matrix with response vector named response
  # train.size
  # OUTPUT1: data frame with pred.date, lwr95, pred, upr95
  # OUTPUT2: data.frame with coefficents of regression
  regress <-function(start.date, train.size, var.names, tdat)
  { 
  # start.date="2013-01-02";train.size=20;var.names=nam;
  
  start<-which(date==start.date)
  regsubset_1=tdat[,var.names]
  regsubset_1<-regsubset_1[start:(start+train.size-1),]
  
  reg<-lm(regsubset_1$response~.,data=regsubset_1)
  
  predsubset<-tdat[(start+train.size),]
  predsubset<-predsubset[,var.names]
  #pred<-predict(reg,predsubset)
  
  pred <- predict(reg, predsubset,  se.fit = TRUE, interval="prediction", level=0.95)
  pred.date<-date[(start+train.size)]
  
  pred.df<-data.frame(pred.date, pred$fit)
  names(pred.df)=c("pred.date", "pred", "lwr95", "upr95")
  coefs<-reg$coefficients
  return(list=c(pred.df=list(pred.df),reg.coefs=list(coefs)))
  }
  
# PREDICTION LOOPER
# start.date: string, Date, format = "YYYY-MM-DD"
# e.g. "2016-01-25"
# var.names: vector of strings
# dat: data matrix with response vector named response
# train.size
# pred.num : number of predictions to make / number of rows in tdat after train + start.date
# OUTPUT1: dataframe with 6 cols: pred.date, pred.vec, lwr95.vec, upr95.vec, actual.price, residuals
# OUTPUT2: MSE for all predictions
# OUTPUT3: dataframe with date and coefficients of regression used for making the prediction on date
# OUTPUT4: training size
looper <-function(start.date, train.size, var.names, tdat, pred.num)
{
  # start.date="2015-10-20"; train.size=10; var.names=nam; pred.num=3;
  start<-which(date==start.date)
  pred.num<-min(dim(tdat)[1]-start-train.size,pred.num)
  pred.date<-rep((as.Date("9999-12-31")),pred.num)
  pred.vec<-rep(0,pred.num)
  lwr95.vec<-rep(0,pred.num)
  upr95.vec<-rep(0,pred.num)
  all.predictions<-data.frame(pred.date,pred.vec,lwr95.vec,upr95.vec)
  
  # run a regression just to get the coefficient names
  reg.for.names<-regress(start.date=date[start],train.size=train.size,var.names=var.names,tdat=tdat)
  all.coefficients = data.frame(t(rep(0,length(reg.for.names$reg.coefs))))
  names(all.coefficients)=names(reg.for.names$reg.coefs)
  
  # names(all.coefficients)=c("intercept",nam)
  for (i in 1: (pred.num+1))
  {
    # date/predictions/CI's
    start.date.loop<-date[start+i-1]
    reg<-regress(start.date=start.date.loop,train.size=train.size,var.names=var.names,tdat=tdat)  
    
    all.predictions[i,]<-reg$pred.df
    
    # Coefficients
    all.coefficients[i,]<-reg$reg.coefs
  }
  
  actual.price<-rep(0,pred.num)
  w<-which(date %in% all.predictions$pred.date)
  actual.price<-tdat$USD.CAD.Close[w]
  residuals<-actual.price-all.predictions$pred.vec
  
  pred.df<-data.frame(all.predictions,actual.price,residuals)
  
  MSE<-sqrt(sum((pred.df$residuals)^2)/length(pred.df$residuals))
  
  coef.df=data.frame(pred.date=all.predictions$pred.date,all.coefficients)
  looper.output<-list(pred.df=pred.df,MSE=MSE,coef.df=coef.df,train.size=train.size)
  return(looper.output)
}

# PLOTTER
# INPUT: output object from looper
# produces plot
####
plotter <- function(looper.o)
{
  df<-looper.o$pred.df
  t<-dim(df)[1]
  a=min(df$actual,df$pred.vec,df$lwr95.vec)
  b=max(df$actual,df$pred.vec,df$upr95.vec)
  plot(df$pred.date,df$actual,type='l',ylim=c(a,b), col='red',lwd=2, 
       main=paste(df$pred.date[1], "to", df$pred.date[t], "\n CVRMSE=", round(looper.o$MSE,10),
                  "\n training size=", looper.o$train.size),
       xlab="Date",ylab="USD/CAD",
       sub=paste("Variables=", length(names(looper.o$coef.df)[-2:-1]))) # actual
  lines(df$pred.date,df$pred.vec,type='l',col='blue') #predicted,
  points(df$pred.date,df$pred.vec,pch=4,col='blue')
  points(df$pred.date,df$actual.price,pch='*',col='red')
  abline(v=df$pred.date,lty=3)
  lines(df$pred.date,df$lwr95.vec)
  lines(df$pred.date,df$upr95)
}
@
  
We perform a test analagous to leave one out cross validation. We various training sizes (100,200,300,400,500,600) in order fit a model that is used to test and perform a prediction interval on the first day following the last day of the training set. We plot lines representing the actual price of the USD/CAD exchange (red), our prediction for that day (blue) and a 95\% confidence interval for the prediction (black). As well we have a MSE, similar to the CVRMSE of cross validation, calculated as the square root of the sum of the errors divided by the number of predictions. Here, the errors are defined as the difference between the prediction and the price of the actual USD/CAD exchange on the date. 

In summary, our prediction system performs \emph{one regression for each prediction}. We experimented with various training sizes. The following plots show the predictions/CVRMSE the same time period (2015-11-10 to 2016-01-25) for various training sizes. The coefficients of regression are unique for each prediction, and the function $looper$ (see appendix for source code), keeps track of the coefficients. We toyed with the idea of analyzing these regression coefficients as a time series, and found that they are of an autoregressive nature.


\SweaveOpts{width=15,height=10}
\begin{figure}
\begin{center}
<<fig=TRUE,include=TRUE,echo=FALSE>>=
ap<-looper(start.date="2013-01-02", train.size=100, var.names=modeladj, tdat, pred.num=100)
plotter(ap)
legend(as.Date("2013-10-01"),1.065,c("actual","predicted","95%"),lwd=c(2,1,1),col=c('red','blue','black'),pch=list('*','4',''))
names(ap$coef.df)[-2:-1]
@
\end{center}
\caption{Plot of prediction vs time on top of actual vs time of exhaustive search best model with training size as 100}
\end{figure}

\SweaveOpts{width=15,height=10}
\begin{figure}
\begin{center}
<<fig=TRUE,include=TRUE,echo=FALSE>>=
ap<-looper(start.date="2013-01-02", train.size=100, var.names=names(tdat)[-1], tdat, pred.num=100)
plotter(ap)
names(ap$coef.df)[-2:-1]
@
\end{center}
\caption{Plot of prediction vs time on top of actual vs time of full model with training size as 100}
\end{figure}

\SweaveOpts{width=15,height=15}
\begin{figure}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
start.date.calculator <- function(pred.date.start,training.size.vec)
{
  start<-which(date==pred.date.start)
  l<-length(training.size.vec)
  pred.date.vec<-rep((as.Date("9999-12-31")),l)
  if (sum((date %in% as.Date(pred.date.start))==TRUE)==1){
    
    for (i in 1:l)
    {
      pred.date.vec[i]<-date[start-training.size.vec[i]]
    }
    
  }
  return (pred.date.vec);
}

v.sizes<-c(100,200,300,400,500,600)
v.dates<-start.date.calculator(pred.date.start="2015-11-10",training.size.vec=v.sizes)
#v.dates

# jpeg(filename = "rPLOT.jpeg", width=3000,height=3000)
par(mfrow=c(3,2))
for (i in 1:length(v.dates))
{
  a<-looper(start.date=v.dates[i], train.size=v.sizes[i], var.names=modeladj, tdat, pred.num=50)
  plotter(a)
}
# dev.off()
@
\end{center}
\caption{Plot of prediction vs time on top of actual vs time of for various training sizes cross validation exhaustive search best model, blue line: prediction, red line: actual prices, black lines: future prediction interval}
\end{figure}

\begin{figure}
\SweaveOpts{width=15,height=15}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
v.sizes<-c(100,200,300,400,500,600)
v.dates<-start.date.calculator(pred.date.start="2015-11-10",training.size.vec=v.sizes)
#v.dates

# jpeg(filename = "rPLOT.jpeg", width=3000,height=3000)
par(mfrow=c(3,2))
for (i in 1:length(v.dates))
{
  a<-looper(start.date=v.dates[i], train.size=v.sizes[i], var.names=names(tdat)[-1], tdat, pred.num=50)
  plotter(a)
}
# dev.off()
@
\end{center}
\caption{Plot of prediction vs time on top of actual vs time of for various training sizes cross validation for full model, blue line: prediction, red line: actual prices, black lines: future prediction interval}
\end{figure}
\section{Summary}

We performed a form $k$-fold cross-validation on the full model with all explanatory variables and on the model chosen by an exhaustive search. We found the model with the exhaustive search had the best adjusted $R^2$, the lowest $C_p$ and the lower CVRMSE. Therefore this is the best model, with 8 linear explanatory variables.

<<>>==
modeladj
@

\end{document}
